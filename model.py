# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZPK429yV8tG9jauhTSweOpCK7QGsQehb
"""

# -*- coding: utf-8 -*-
"""model.ipynb"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
import pytorch_lightning as pl

# -------------------------------------------------------------------
# 1. Helper Module: MLP for Landmarks
# -------------------------------------------------------------------
class MLPLandmark(nn.Module):
    def __init__(self):
        super(MLPLandmark, self).__init__()
        # Input features: 19 landmark-related values
        self.mlp = nn.Sequential(
            nn.Linear(19, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU()
        )

    def forward(self, landmark_data):
        return self.mlp(landmark_data)

# -------------------------------------------------------------------
# 2. Helper Module: SmallCNN for Eye Images
# -------------------------------------------------------------------
class SmallCNN(nn.Module):
    def __init__(self, in_channels=3, output_features=64):
        super(SmallCNN, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        # Flattened size calculation:
        # Input: 50x100 -> Pool1 -> 25x50 -> Pool2 -> 12x25 -> Pool3 -> 6x12
        # 128 channels * 6 * 12 = 9216
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128 * 6 * 12, output_features),
            nn.ReLU()
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

# -------------------------------------------------------------------
# 3. Main Model: DashGazeNetMini
# -------------------------------------------------------------------
class DashGazeNetMini(nn.Module):
    def __init__(self,
                 resnet_output_features=512, # Features from ResNet18 after avgpool
                 smallcnn_output_features=64,
                 mlp_output_features=32,
                 shared_fc_dim=256):
        super(DashGazeNetMini, self).__init__()

        # A. ResNet18 for 'face' image
        self.resnet_face = models.resnet18(pretrained=True)
        self.resnet_face.fc = nn.Identity() # Remove final FC layer

        # B. ResNet18 for 'driver' image
        self.resnet_driver = models.resnet18(pretrained=True)
        self.resnet_driver.fc = nn.Identity() # Remove final FC layer

        # C. SmallCNN for 'leye' image
        self.smallcnn_leye = SmallCNN(in_channels=3, output_features=smallcnn_output_features)

        # D. SmallCNN for 'reye' image
        self.smallcnn_reye = SmallCNN(in_channels=3, output_features=smallcnn_output_features)

        # E. MLPLandmark for landmark data
        self.mlp_landmark = MLPLandmark()

        # Calculate total features after concatenation
        total_concat_features = (
            resnet_output_features * 2 + # 512 * 2
            smallcnn_output_features * 2 + # 64 * 2
            mlp_output_features # 32
        ) # Total = 1024 + 128 + 32 = 1184

        # F. Shared fully connected layer
        self.shared_fc = nn.Sequential(
            nn.Linear(total_concat_features, shared_fc_dim),
            nn.ReLU()
        )

        # G. Output heads
        self.gaze_angle_head = nn.Linear(shared_fc_dim, 2)    # (yaw, pitch)
        self.gaze_location_head = nn.Linear(shared_fc_dim, 2) # (x, y)

    def forward(self, driver_img, face_img, leye_img, reye_img, landmark_data):
        # Extract features
        features_face = self.resnet_face(face_img)      # Shape: [B, 512]
        features_driver = self.resnet_driver(driver_img) # Shape: [B, 512]
        features_leye = self.smallcnn_leye(leye_img)    # Shape: [B, 64]
        features_reye = self.smallcnn_reye(reye_img)    # Shape: [B, 64]
        features_landmark = self.mlp_landmark(landmark_data) # Shape: [B, 32]

        # Concatenate
        combined_features = torch.cat([
            features_face,
            features_driver,
            features_leye,
            features_reye,
            features_landmark
        ], dim=1)

        # Shared processing
        shared_output = self.shared_fc(combined_features)

        # Predictions
        gaze_angle_pred = self.gaze_angle_head(shared_output)
        gaze_location_pred = self.gaze_location_head(shared_output)

        return gaze_angle_pred, gaze_location_pred

# -------------------------------------------------------------------
# 4. Lightning Wrapper: GazeEstimationLightningModule
# -------------------------------------------------------------------
class GazeEstimationLightningModule(pl.LightningModule):
    def __init__(self, screen_width=1920, screen_height=1080):
        super().__init__()
        self.model = DashGazeNetMini()
        self.loss_fn = nn.MSELoss()
        self.lambda_loss = 1.0

        # Store screen dims for normalization calculations
        self.screen_width = screen_width
        self.screen_height = screen_height

    def forward(self, driver_img, face_img, leye_img, reye_img, landmark_data):
        return self.model(driver_img, face_img, leye_img, reye_img, landmark_data)

    def angular_error(self, gaze_angle_pred, gaze_angle_target):
        # Convert degrees to radians
        pred_rad = torch.deg2rad(gaze_angle_pred)
        target_rad = torch.deg2rad(gaze_angle_target)

        # Convert to 3D Cartesian vectors
        pred_x = torch.cos(pred_rad[:, 1]) * torch.cos(pred_rad[:, 0])
        pred_y = torch.cos(pred_rad[:, 1]) * torch.sin(pred_rad[:, 0])
        pred_z = torch.sin(pred_rad[:, 1])
        pred_vectors = torch.stack([pred_x, pred_y, pred_z], dim=1)

        target_x = torch.cos(target_rad[:, 1]) * torch.cos(target_rad[:, 0])
        target_y = torch.cos(target_rad[:, 1]) * torch.sin(target_rad[:, 0])
        target_z = torch.sin(target_rad[:, 1])
        target_vectors = torch.stack([target_x, target_y, target_z], dim=1)

        # Dot product
        dot_product = torch.sum(pred_vectors * target_vectors, dim=1)
        dot_product = torch.clamp(dot_product, -1.0 + 1e-7, 1.0 - 1e-7)

        # Angle in degrees
        angle_degrees = torch.rad2deg(torch.acos(dot_product))
        return angle_degrees.mean()

    def location_error(self, gaze_location_pred, gaze_location_target):
        # Euclidean distance in pixels
        error = torch.sqrt(torch.sum((gaze_location_pred - gaze_location_target)**2, dim=1))
        return error.mean()

    def normalized_location_error(self, gaze_location_pred, gaze_location_target):
        # 1. Calculate Pixel Error
        pixel_dist = torch.sqrt(torch.sum((gaze_location_pred - gaze_location_target)**2, dim=1))
        # 2. Normalize by screen width
        norm_error = pixel_dist / self.screen_width
        return norm_error.mean()

    def _common_step(self, batch, batch_idx, stage: str):
        # Unpack dictionary from datamodule
        driver_img = batch['driver']
        face_img = batch['face']
        leye_img = batch['leye']
        reye_img = batch['reye']
        landmark_data = batch['landmarks']

        gaze_angle_target = batch['gaze_ang']
        gaze_location_target = batch['gaze_loc']

        # Forward
        gaze_angle_pred, gaze_location_pred = self(driver_img, face_img, leye_img, reye_img, landmark_data)

        # Loss Calculation
        loss_angle = self.loss_fn(gaze_angle_pred, gaze_angle_target)
        loss_location = self.loss_fn(gaze_location_pred, gaze_location_target)
        total_loss = loss_angle + self.lambda_loss * loss_location

        # --- LOGGING ---
        # 1. Total Loss (Optimization objective)
        self.log(f'{stage}_total_loss', total_loss, on_step=False, on_epoch=True, prog_bar=True)

        # 2. Raw MSE Losses (For debugging/comparison with old runs)
        self.log(f'{stage}_gaze_angle_mse', loss_angle, on_step=False, on_epoch=True)
        self.log(f'{stage}_gaze_loc_mse', loss_location, on_step=False, on_epoch=True)

        # 3. Human-Readable Metrics (Degrees & Pixels)
        avg_angular_error = self.angular_error(gaze_angle_pred, gaze_angle_target)
        avg_location_error = self.location_error(gaze_location_pred, gaze_location_target)

        self.log(f'{stage}_angular_error', avg_angular_error, on_step=False, on_epoch=True, prog_bar=True)
        self.log(f'{stage}_location_error', avg_location_error, on_step=False, on_epoch=True, prog_bar=True)

        # 4. Normalized Metrics (For Research Paper/Quantitative Testing)
        # We only strictly need this during validation/testing
        if stage in ['val', 'test']:
            avg_norm_error = self.normalized_location_error(gaze_location_pred, gaze_location_target)
            self.log(f'{stage}_norm_location_error', avg_norm_error, on_step=False, on_epoch=True, prog_bar=True)

        return total_loss

    def training_step(self, batch, batch_idx):
        return self._common_step(batch, batch_idx, 'train')

    def validation_step(self, batch, batch_idx):
        return self._common_step(batch, batch_idx, 'val')

    def test_step(self, batch, batch_idx):
        return self._common_step(batch, batch_idx, 'test')

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=1e-4)